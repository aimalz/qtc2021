{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJJCNEYMp64R"
   },
   "source": [
    "# Assessing the accuracy of ML-based uncertainties in the context of galaxy photometric redshifts\n",
    "\n",
    "_Alex I. Malz (German Centre for Cosmological Lensing)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2O19PzXur-GF"
   },
   "source": [
    "## Motivation: Uncertainty quantification\n",
    "\n",
    "More and more ML/AI methods, particularly in Bayesian deep learning, yield uncertainties $\\hat{p}(y | x_{i})$ of target parameters $y$ given observed random variables $x_{i}$, rather than just point estimates $\\hat{y}_{i}$.\n",
    "Though rarely framed as such, these uncertainties are _posteriors_, and there's more than meets the eye to what lies on the righthand side of the conditional.\n",
    "\n",
    "Really, an estimated posterior should be writen as $p(y | x_{i}, \\{y_{n}, x_{n}\\}_{N}, \\pi)$ for training data $\\{y_{n}, x_{n}\\}_{N}$ and algorithm $\\pi$.\n",
    "Though the dependence on training data is straightforward, the dependence on the algorithm, meaning the estimation model and its implementation, is subtle.\n",
    "However, if it were not there, then every method trained on the same data would yield identical estimated uncertainties, which is not observed.\n",
    "While the training set $\\{y_{n}, x_{n}\\}_{N}$ is equivalent to an _explicit prior_, the algorithm $\\pi$ must be an _implicit prior_.\n",
    "\n",
    "Another way of looking at estimated posteriors is in terms of the type of uncertainty encompassed by each term on the righthand side of the conditional.\n",
    "For a noisy measurement or a stochastic generative process, the random variable $x_{i} \\sim p(x | y_{i})$ represents the _aleatoric uncertainty_, that inherent to the data.\n",
    "However, the training set $\\{y_{n}, x_{n}\\}_{N}$ and implemented algorithm $\\pi$ could potentially be improved to yield a better estimate and thus constitute sources of _epistemic uncertainty_.\n",
    "In physics, we want to learn the aleatoric uncertainty $p(x | y_{i})$, which we can't get from the $p(y | x_{i}, \\{y_{n}, x_{n}\\}_{N}, \\pi)$ in hand without knowing $p(y | \\{y_{n}, x_{n}\\}_{N}, \\pi)$.\n",
    "\n",
    "Meanwhile, assessments of the performance of estimated $p(y | x_{i}, \\{y_{n}, x_{n}\\}_{N}, \\pi)$ are almost always made by comparison to known $y_{i}$, leaving unanswered the question of how well the estimator approximates $p(y | x_{i})$.\n",
    "Why?\n",
    "The problem is that $p(y | x_{i})$ is not necessarily known, certainly not for observed $y_{i}$ measured in nature, but also generally for simulated $y_{i}$, at least in astrophysics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Pgc2EBKsJ6X"
   },
   "source": [
    "## tl;dr Goals\n",
    "\n",
    "This data challenge invites participants to address any of the following open problems.\n",
    "- How can we use AI to estimate posteriors?\n",
    "- How should we assess posterior estimates?\n",
    "- How may we quantify the implicit prior(s)?\n",
    "\n",
    "This tutorial/demo provides some starting points for answering these questions in the context of photometric redshifts (photo-$z$s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QW-T6Zq70MwL"
   },
   "source": [
    "## Context: Photometric redshifts\n",
    "\n",
    "[Photo-$z$s](https://en.wikipedia.org/wiki/Photometric_redshift) provide an excellent testbed for addressing these issues but require some introduction.\n",
    "There isn't a definitive primer, but here are several overviews that are informative, if a bit dry.\n",
    "- [basic intro from Rubin Observatory](https://www.lsst.org/science/dark-energy/photometric-redshift)\n",
    "- [old overview covering classic concepts](https://ned.ipac.caltech.edu/level5/Glossary/Essay_photredshifts.html)\n",
    "- [recent review of estimation methods](https://arxiv.org/abs/1805.12574)\n",
    "If these don't answer your questions, Chapter 0 of [my thesis](https://zenodo.org/record/3973536) might, or, better yet, [the slides from my defense](https://github.com/aimalz/ship-of-theses/tree/master/presentation) make for a better tl;dr.\n",
    "\n",
    "One reason photo-$z$s are perfect for a data challenge is that they are simple enough to obtain $p(y | x_{i})$ along the way to generating a sample of $(y_{i}, x_{i})$ pairs.\n",
    "Here, the target variable $y \\to z$ is redshift, a scalar, and the data $x \\to \\vec{d} = (u, g, r, i, z, y)$, or some trivial function thereof, is a vector of length $<10$ observed [photometric magnitudes](https://en.wikipedia.org/wiki/Magnitude_(astronomy)) of galaxies through [broadband optical filters](https://en.wikipedia.org/wiki/Photometric_system) (which I somewhat arbitrarily choose to be those of [the Vera C. Rubin Observatory](https://www.lsst.org/)).\n",
    "Because of the extremely low dimensionality of the problem, we can forward model not just individual pairs $(z_{i}, \\vec{d}_{i})$ but the entire joint probability space $p(z, \\vec{d})$, thereby obtaining $p(y | x_{i})$ for every $(z_{i}, \\vec{d}_{i})$.\n",
    "That doesn't mean it's trivial to do so -- most of this tutorial concerns that forward-modeling procedure -- but it is possible.\n",
    "\n",
    "Another reason photo-$z$s are perfect for this data challenge is that comprehensive uncertainty quantification for galaxy redshifts in the absence of spectroscopy are crucial for the Legacy Survey of Space and Time (LSST), an upcoming photometric survey on the Rubin Observatory.\n",
    "This data challenge has the ulterior motive of strengthening the cosmology analysis pipeline of LSST's Dark Energy Science Collaboration (DESC).\n",
    "This tutorial makes use of two pieces of software DESC members are publicly developing right now, [RAIL](https://github.com/LSSTDESC/RAIL) and [qp](https://github.com/LSSTDESC/qp), whose functionality will be introduced where relevant, along with other code dependencies.\n",
    "Participants need not use either in their responses to the challenge questions, but the development team welcomes feedback from potential users, contributions to the codebase that could result from this data challenge, and new team members, DESC membership not required.\n",
    "_If your solutions to the challenge lead to any publications, you must cite RAIL and qp._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIOag5iVqZPW"
   },
   "source": [
    "# Tutorial and challenge prompts\n",
    "\n",
    "Finally, without more ado, we set the stage with a tutorial that:\n",
    "1. creates realistically complex mock photo-$z$ posteriors, redshifts, and photometry to define training and test sets;\n",
    "2. estimates photo-$z$ posteriors of the test set given the training set using ML;\n",
    "3. quantifies how closely the estimated posteriors approximate the true ones.\n",
    "\n",
    "_The three-pronged structure of this tutorial is inspired by that of RAIL, which has subpackages corresponding to each of the enumerated parts of the tutorial: `rail.creation`, `rail.estimation`, and `rail.evaluation`._\n",
    "\n",
    "The challenge comes from building on the tutorial content to conduct a self-guided investigation of the open questions.\n",
    "To get the most out of this opportunity, think of this tutorial as a lab manual that presents an experimental procedure to answer each question and includes an example of a possible solution, then invites participants to \"riff\" on those solutions to devise and implement their own.\n",
    "The greatest opportunities for such opportunities can be found under the heading \"Your turn!\" throughout the tutorial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JituVGNVZ7bg"
   },
   "source": [
    "## 0. Setup\n",
    "\n",
    "This tutorial uses several packages that may not be installed in the conference's kernel.\n",
    "The following commands installed them successfully on Google Colab, but there might be a better way to install it on PSC machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_2llNhgut6jM",
    "outputId": "41d9f8cd-1b90-4e45-91f6-6d9c2f6a4d75"
   },
   "outputs": [],
   "source": [
    "# !pip install cde-diagnostics\n",
    "# !pip install cdetools\n",
    "# !pip install corner\n",
    "# !pip install FlexCode\n",
    "# !pip install pzflow\n",
    "# !pip install sklearn\n",
    "# !pip install xgboost==0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNhx0u1vszZN"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lFFWI08yYKFV",
    "outputId": "3bbbe140-7c98-4316-87ae-45e4383943f5"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/COINtoolbox/photoz_catalogues.git\n",
    "# !pip install git+https://github.com/LSSTDESC/RAIL.git#egg=rail[Full]\n",
    "# !pip install git+https://github.com/LSSTDESC/qp.git#egg=qp[Full]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XHM0pdtvLdG2",
    "outputId": "fa437d28-233f-400c-8d06-040bfec6e2fd"
   },
   "outputs": [],
   "source": [
    "import qp\n",
    "import rail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZs_a1wy0EbC"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpl_patches\n",
    "mpl.rc('text', usetex=False)\n",
    "\n",
    "import corner\n",
    "\n",
    "from cycler import cycler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9imLKSxZ_F1"
   },
   "source": [
    "## 1. Generating test and training data, including true photo-$z$ posteriors\n",
    "\n",
    "This is the most complex of the parts of the tutorial, which should leave readers with a sense of why this kind of experiment has not been done for higher-dimensional data.\n",
    "There are three main steps:\n",
    "0. Prepare and explore data to use as the basis for a realistically complex generative model.\n",
    "1. Emulate a realistically complex $p(z, \\vec{d})$ probability space using input data.\n",
    "2. Emulate a distinct $p'(z, \\vec{d})$ probability space from an existing model $p(z, \\vec{d})$ of that space.\n",
    "3. Sample $p(z, \\vec{d})$ and $p'(z, \\vec{d})$ to produce test and training sets, respectively, with known $z$, $\\vec{d}$, and $p(z | \\vec{d})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pexhHnZsoNq"
   },
   "source": [
    "### Data\n",
    "\n",
    "Though we're going to forward-model mock data to experiment on, we want it to be _realistically complex_, meaning it shares the physical degeneracies and systematic errors that would be present in a real data set.\n",
    "The [Happy/Teddy data sets](https://github.com/COINtoolbox/photoz_catalogues) (see [Beck, et al 2017](https://arxiv.org/abs/1701.08748) for full release notes) are curated subsamples of the [Sloan Digital Sky Survey (SDSS) Data Release (DR) 12](https://www.sdss.org/dr12/), a spectroscopic survey with high-fidelity redshift measurements, and were created by the [Cosmostatistics Initiative (COIN)](https://cosmostatistics-initiative.org/).\n",
    "The data sets are defined to emulate the kinds of differences in observational properties of galaxies with measured spectroscopic redshifts and those for which only photometry is available, and they were created with the goal of determining the impact of imbalance between training, validation, and test sets for photo-$z$ point estimation.\n",
    "They are thus an appropriate starting point for creating a realistically complex model of the joint probability space $p(z, \\vec{d})$.\n",
    "\n",
    "Of course there are plenty of other potential data sets available, including those that are simulated, which may be advantageous for the potential to estimate other galaxy properties such as stellar mass and star formation rate that are known in a simulation but not directly measurable with real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71MdMCuXn16X"
   },
   "outputs": [],
   "source": [
    "happy_path = '../photoz_catalogues/Happy/happy_'\n",
    "header = pd.read_csv(happy_path+'A', delim_whitespace=True, nrows=0).columns[1:]\n",
    "teddy_path = '../photoz_catalogues/Teddy/teddy_'\n",
    "\n",
    "happy, teddy = {}, {}\n",
    "for lett in ['A', 'B', 'C', 'D']:\n",
    "    happy[lett] = pd.read_csv(happy_path+lett, delim_whitespace=True, header=None, skiprows=1, names=header)\n",
    "    happy[lett] = happy[lett]\n",
    "    teddy[lett] = pd.read_csv(teddy_path+lett, delim_whitespace=True, header=None, skiprows=7, names=header)\n",
    "    teddy[lett] = teddy[lett]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "TV_VxG8CtoDW",
    "outputId": "50e07d68-aec6-4b18-ab5c-eb9fbeb0cd66"
   },
   "outputs": [],
   "source": [
    "# TODO: separate Happy and Teddy\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "for j, col in enumerate(['z_spec', 'mag_r']):\n",
    "    for i, lett in enumerate(['A', 'B', 'C', 'D']):\n",
    "        ax[j][0].hist(happy[lett][col], alpha=0.25, bins=100, density=False, label='happy_'+lett)\n",
    "        ax[j][1].hist(teddy[lett][col], alpha=0.25, bins=100, density=False, label='teddy_'+lett)\n",
    "        ax[j][0].set_xlabel(col)\n",
    "        ax[j][0].legend()\n",
    "        ax[j][1].set_xlabel(col)\n",
    "        ax[j][1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience later on, we can safely cut off the tiny fraction of outliers in redshift and $r$-band magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WutxjLl4YvLG"
   },
   "outputs": [],
   "source": [
    "z_min, z_max = 0., 1.5\n",
    "r_min, r_max = 10., 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9YMBF_NsekS"
   },
   "outputs": [],
   "source": [
    "colorcycle = 'rbgcmy'\n",
    "fig = corner.corner(happy['A'][['u-g', 'g-r', 'r-i', 'i-z']], color='k', alpha=0.25)\n",
    "for i, lett in enumerate(['B', 'C', 'D']):\n",
    "    corner.corner(happy[lett][['u-g', 'g-r', 'r-i', 'i-z']], fig=fig, color=colorcycle[i], alpha=0.25)\n",
    "    \n",
    "fig = corner.corner(teddy['A'][['u-g', 'g-r', 'r-i', 'i-z']], color='k', alpha=0.25)\n",
    "for i, lett in enumerate(['B', 'C', 'D']):\n",
    "    corner.corner(teddy[lett][['u-g', 'g-r', 'r-i', 'i-z']], fig=fig, color=colorcycle[i], alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuzPDc5WsrUK"
   },
   "source": [
    "### Emulating $p(z, \\vec{d})$ from input data with `pzflow`\n",
    "\n",
    "[`pzflow`](https://github.com/jfcrenshaw/pzflow) is a package for making normalizing flows from sets of redshifts and photometry in order to estimate or otherwise model photo-$z$ posteriors.\n",
    "We'll use it to make a normalizing flow that will serve as the model for $p(z, \\vec{d})$.\n",
    "This is the only part of the tutorial that explicitly takes advantage of GPU capabilities.\n",
    "\n",
    "_This content is adapted from pzflow's [conditional redshifts demo](https://github.com/jfcrenshaw/pzflow/blob/main/examples/conditional_redshift_example.ipynb), written by John Franklin Crenshaw (UW)._\n",
    "_The same functionality can also be accessed through a `rail.Creator()` object._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yW3ONw4q9pw"
   },
   "outputs": [],
   "source": [
    "#import pzflow\n",
    "from pzflow import Flow\n",
    "from pzflow.bijectors import Chain, ColorTransform, InvSoftplus, StandardScaler, RollingSplineCoupling\n",
    "from pzflow.examples import galaxy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxmVog1_kTqu"
   },
   "outputs": [],
   "source": [
    "data = galaxy_data()\n",
    "\n",
    "# restrict to Happy/Teddy range for coverage in demo\n",
    "data = data[(data['redshift'] > z_min) & (data['redshift'] < z_max) & (data['r'] > r_min) & (data['r'] < r_max)]\n",
    "\n",
    "# normalize\n",
    "data = data.values\n",
    "\n",
    "# use fewer bands to be able to compare with Happy/Teddy\n",
    "data = data[:, :-1]\n",
    "\n",
    "#     # # convert magnitudes to a reference magnitude and colors\n",
    "data[:, 1], data[:, 2:] = data[:, 3], np.diff(data[:,1:])\n",
    "\n",
    "    # standard scale the reference magnitude and colors\n",
    "data[:, 1:] = (data[:, 1:] - data[:, 1:].mean(axis=0))/data[:, 1:].std(axis=0)\n",
    "\n",
    "# calculate the mean and standard deviations of the dimensions\n",
    "# note that on this side of the color transform, we are working\n",
    "# in color space, so I calculate the mean and std dev of these \n",
    "# variables instead of the raw magnitudes\n",
    "means = data[:, 1:].mean(axis=1)\n",
    "stds = data[:, 1:].std(axis=1)\n",
    "\n",
    "# save the new set\n",
    "data = pd.DataFrame(data, columns=('redshift', 'r', 'u-g', 'g-r', 'r-i', 'i-z'))\n",
    "# trim_data[i][lett] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "4KdppeUD5t7l",
    "outputId": "905f9ff8-f4b3-4e58-db71-5c961a9c4c40"
   },
   "outputs": [],
   "source": [
    "# set the inverse softplus parameters, estimated\n",
    "# to ensure that sampled redshifts are positive\n",
    "column_idx = 0\n",
    "sharpness = 5\n",
    "n_epoch = 125\n",
    "\n",
    "# construct our bijector\n",
    "# by chaining all these layers\n",
    "bijector = Chain(\n",
    "    InvSoftplus(column_idx, sharpness),\n",
    "    StandardScaler(means[column_idx], stds[column_idx]),\n",
    "    RollingSplineCoupling(nlayers=1, n_conditions=len(data.columns)-1),\n",
    ")\n",
    "\n",
    "# To create the conditional flow, we have to provide\n",
    "# 1. The names of the data columns\n",
    "# 2. The bijector\n",
    "# 3. The names of the conditional columns\n",
    "flow = Flow(data.columns[:1], bijector, conditional_columns=data.columns[1:])\n",
    "\n",
    "losses = flow.train(data, epochs=n_epoch, verbose=True)\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNl-Q4YqXaxc"
   },
   "outputs": [],
   "source": [
    "granularity = 100\n",
    "grid = np.linspace(z_min, z_max, granularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "qS_Zs9Vqbs3x",
    "outputId": "85c91abd-38e8-4ac9-b73a-e8b5bc9b5a2e"
   },
   "outputs": [],
   "source": [
    "chosen = 999\n",
    "\n",
    "galaxy = data.loc[[chosen]]\n",
    "pdf = flow.posterior(galaxy, column=\"redshift\", grid=grid)\n",
    "\n",
    "plt.plot(grid, pdf[0], label='Posterior')\n",
    "plt.axvline(galaxy['redshift'].values[0], 0, 1, c='C3', label='True redshift')\n",
    "plt.legend()\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rndWVcui_f6"
   },
   "outputs": [],
   "source": [
    "# # draw 4 samples for each of the first 500 conditions\n",
    "# samples = flow.sample(4, conditions=data[:500], seed=0)\n",
    "\n",
    "# # lets print the first 10\n",
    "# # see how conditions 0-3 are the same?\n",
    "# # and 4-7? etc.\n",
    "# print(samples[:10])\n",
    "\n",
    "\n",
    "# # print the shape of the samples\n",
    "# # see we have 2000 of them!\n",
    "# print(samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "OGrTo5J4TKuQ",
    "outputId": "563c1549-0340-4124-e5ab-cefb70f7e8a6"
   },
   "outputs": [],
   "source": [
    "samples = flow.sample(1, conditions=data[:5000], seed=0)\n",
    "plt.hist(data['redshift'], range=(0, 2.5), bins=40, histtype='step', label='data', density=True)\n",
    "plt.hist(samples['redshift'], range=(0, 2.5), bins=40, histtype='step', label='samples', density=True)\n",
    "plt.xlabel('z')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwGykJnbmqys"
   },
   "outputs": [],
   "source": [
    "z = samples['redshift']\n",
    "z.to_csv('test_set_redshifts.csv')\n",
    "\n",
    "phot = samples[['r', 'u-g', 'g-r', 'r-i', 'i-z']]\n",
    "phot.to_csv('test_set_photometry.csv')\n",
    "\n",
    "posteriors = flow.posterior(samples, column=\"redshift\", grid=grid)\n",
    "with open('test_set_posteriors.csv', 'wb') as fn:\n",
    "    jnp.save(fn, posteriors)\n",
    "\n",
    "# TODO: change posteriors to .npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyecUsXrJJ7R"
   },
   "source": [
    "We can do this procedure for all the Happy/Teddy samples so we can experiment with them later.\n",
    "**TODO: plot the color-color space of samples vs. original, some example true posteriors, maybe MAP plot from RAIL creation demo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dvYgIMnWuq9b",
    "outputId": "f51a1687-653a-40d0-db10-bd5545065208"
   },
   "outputs": [],
   "source": [
    "full_data = {'happy': happy, 'teddy': teddy}\n",
    "n_out = 1000\n",
    "\n",
    "for name, dat in full_data.items():\n",
    "    for lett in ['A', 'B', 'C', 'D']:\n",
    "        print(name+lett)\n",
    "        dat[lett] = dat[lett][['z_spec', 'mag_r', 'u-g', 'g-r', 'r-i', 'i-z']]\n",
    "        dat[lett] = dat[lett][(dat[lett]['z_spec'] > z_min) & (dat[lett]['z_spec'] < z_max) & (dat[lett]['mag_r'] > r_min) & (dat[lett]['mag_r'] < r_max)]\n",
    "        dat[lett] = dat[lett].rename(columns={'z_spec': 'redshift', 'mag_r': 'r'})\n",
    "  \n",
    "        data = dat[lett].values\n",
    "\n",
    "    # standard scale the reference magnitude and colors\n",
    "        data[:, 1:] = (data[:, 1:] - data[:, 1:].mean(axis=0))/data[:, 1:].std(axis=0)\n",
    "\n",
    "    # calculate the mean and standard deviations of thefrom sklearn.cross_validation import train_test_split dimensions\n",
    "    # note that on this side of the color transform, we are working\n",
    "    # in color space, so I calculate the mean and std dev of these \n",
    "    # variables instead of the raw magnitudes\n",
    "        means = data[:, 1:].mean(axis=0)\n",
    "        stds = data[:, 1:].std(axis=0)\n",
    "\n",
    "    # save the new set\n",
    "        data = pd.DataFrame(data, columns=('redshift', 'r', 'u-g', 'g-r', 'r-i', 'i-z'))\n",
    "\n",
    "    # construct our bijector\n",
    "  # by chaining all these layers\n",
    "        bijector = Chain(\n",
    "          InvSoftplus(column_idx, sharpness),\n",
    "          StandardScaler(means[column_idx], stds[column_idx]),\n",
    "          RollingSplineCoupling(nlayers=1, n_conditions=len(data.columns)-1),\n",
    "        )\n",
    "\n",
    "  # To create the conditional flow, we have to provide\n",
    "  # 1. The names of the data columns\n",
    "  # 2. The bijector\n",
    "  # 3. The names of the conditional columns\n",
    "        flow = Flow(data.columns[:1], bijector, conditional_columns=data.columns[1:])\n",
    "\n",
    "        losses = flow.train(data, epochs=n_epoch, verbose=False)\n",
    "\n",
    "        samples = flow.sample(1, conditions=data.sample(n_out), seed=0)\n",
    "\n",
    "        z = samples['redshift']\n",
    "        z.to_csv(name+lett+'redshifts.csv', index=False)\n",
    "\n",
    "        phot = samples[['r', 'u-g', 'g-r', 'r-i', 'i-z']]\n",
    "        phot.to_csv(name+lett+'photometry.csv', index=False)\n",
    "\n",
    "        posteriors = flow.posterior(samples, column=\"redshift\", grid=grid)\n",
    "        with open(name+lett+'posteriors.csv', 'wb') as fn:\n",
    "            jnp.save(fn, posteriors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZsWWSsss9gd"
   },
   "source": [
    "### Emulating $p'(z, \\vec{d})$ from $p(z, \\vec{d})$ using `rail.creation`\n",
    "\n",
    "While one can build $p(z, \\vec{d})$ from an input data set, more systematic investigation can be conducted by creating many training, validation, and test set combinations that are incremental variations on one another.\n",
    "As one of RAIL's purposes is to conduct one such experiment, the `rail.creation` subpackage not only wraps `pzflow` (the above example can be executed just as easily through the RAIL interface) but also includes physically motivated forms of systematic discrepancy between training and test sets that can be applied to a tunable degree to create \"degraded\" versions of an original $p(z, \\vec{d})$ space.\n",
    "So, for completeness, let's demonstrate how to create a new $p'(z, \\vec{d})$ from an existing $p'(z, \\vec{d})$ using `rail.creation`.\n",
    "\n",
    "_This content is taken directly from RAIL's [degradation demo](), written by John Franklin Crenshaw (UW)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKyAI4Rht6lq"
   },
   "outputs": [],
   "source": [
    "import rail\n",
    "from pzflow.examples import example_flow\n",
    "from rail.creation import Creator, engines\n",
    "from rail.creation.degradation import InvRedshiftIncompleteness, LineConfusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeJTFKFftEF9"
   },
   "outputs": [],
   "source": [
    "flow = engines.FlowEngine(example_flow())\n",
    "creator = Creator(flow)\n",
    "degraded_creator = Creator(flow, degrader=InvRedshiftIncompleteness(0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zq1DeWsouA5q"
   },
   "outputs": [],
   "source": [
    "samples = creator.sample(100000)\n",
    "degraded_samples = degraded_creator.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "YFhiqNLtuEP-",
    "outputId": "f492a651-d11b-4e79-e885-1a995a5482d9"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4.5,4.5), dpi=100)\n",
    "ax.hist(samples['redshift'], bins=20, range=(0,2.3), histtype='step', label=\"Unbiased samples\")\n",
    "ax.hist(degraded_samples['redshift'], bins=20, range=(0,2.3), histtype='step', label=\"Degraded samples\")\n",
    "ax.legend()\n",
    "ax.set(xlabel=\"Redshift\", ylabel=\"Number of galaxies\", xlim=(0,2.3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2w-xKXWuKIs",
    "outputId": "161a48f2-c2ff-457a-a81e-61d142f506c8"
   },
   "outputs": [],
   "source": [
    "degraded_samples.shape == samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "osA7xIw7uNUY"
   },
   "outputs": [],
   "source": [
    "def OxygenLineConfusion(data, seed=None):\n",
    "    OII = 3727\n",
    "    OIII = 5007\n",
    "    \n",
    "    data = LineConfusion(true_wavelen=OII, wrong_wavelen=OIII, frac_wrong=0.02)(data, seed)\n",
    "    data = LineConfusion(true_wavelen=OIII, wrong_wavelen=OII, frac_wrong=0.01)(data, seed)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lToSS4aFuQr3"
   },
   "outputs": [],
   "source": [
    "flow = engines.FlowEngine(example_flow())\n",
    "creator = Creator(flow)\n",
    "degraded_creator = Creator(flow, degrader=OxygenLineConfusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKvCXPj2uXxh"
   },
   "outputs": [],
   "source": [
    "samples = creator.sample(100000, seed=0)\n",
    "degraded_samples = degraded_creator.sample(100000, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "_CJ95opYuZBX",
    "outputId": "b6f1a741-8b01-4624-d210-a6ae4a62b32e"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4.5,4.5), dpi=100)\n",
    "ax.scatter(samples[\"redshift\"], degraded_samples[\"redshift\"], s=0.1)\n",
    "ax.set(xlabel=\"True spec-z\", ylabel=\"Erroneous spec-z\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uJdcwXPv4wD"
   },
   "source": [
    "### Your turn!\n",
    "\n",
    "These are just two ways to create a probability space $p(z, \\vec{d})$ that can forward model redshifts, photometry, and photo-$z$ posteriors.\n",
    "Next, we'll use the samples drawn from these systematically different models as training, validation, and test sets for estimators of photo-$z$ posteriors.\n",
    "\n",
    "**CHALLENGE**: Try modeling $p(z, \\vec{d})$ with different starting data sets.\n",
    "\n",
    "**CHALLENGE**: Make systematically different sample data sets to use as training, validation, and test sets, varying them in controlled ways.\n",
    "\n",
    "**CHALLENGE**: Devise and implement another creator of a joint probability space that can encompass realistic complexity of photometric data.\n",
    "\n",
    "**CHALLENGE**: Extend the probability space to $p(z, SED)$ to draw redshifts, full spectra, and redshift posteriors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xpQw49TaR03"
   },
   "source": [
    "## 2. Estimating photo-z posterior PDFs\n",
    "\n",
    "There are many estimators of photo-$z$ posteriors, and many of those are compared to one another in Schmidt & Malz, et al. 2021. \n",
    "**TODO: cite PZ DC1 paper to direct to already-implemented photo-z posterior estimators**\n",
    "Of the tested estimators, including ML and non-ML methods, the most promising was `FlexCode` **TODO: cite it**, which also happens to be one of the easiest to install and apply, so we'll demonstrate it as an example of an estimator of photo-$z$ posteriors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZsmK6L8v9OO"
   },
   "source": [
    "### Estimating photo-$z$ posterior PDFs with FlexCode\n",
    "\n",
    "A demonstration of FlexCode in the context of photo-$z$s can be found in **TODO: cite paper**, with demos in R published on [GitHub](https://github.com/Mr8ND/cdetools_applications).\n",
    "We'll demonstrate it on the pzflow-based samples generated from the Happy/Teddy data sets.*italicized text*\n",
    "\n",
    "_This content is adapted from FlexCode's [Teddy tutorial](https://github.com/tpospisi/FlexCode/blob/master/tutorial/Flexcode-tutorial-teddy.ipynb), written by Nic Dalmasso (CMU)._\n",
    "_The same functionality can also be accessed through a `rail.Estimator()` object; see [tutorial](https://github.com/LSSTDESC/RAIL/blob/master/examples/estimation/RAIL_estimation_demo.ipynb) written by Sam Schmidt (UC Davis)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGZ8i_opZiFY"
   },
   "outputs": [],
   "source": [
    "n_grid = 100\n",
    "\n",
    "# import jax.numpy as np\n",
    "import flexcode\n",
    "\n",
    "# Select regression method\n",
    "from flexcode.regression_models import NN\n",
    "\n",
    "# Parameters\n",
    "basis_system = \"cosine\"  # Basis system\n",
    "max_basis = 31           # Maximum number of basis. If the model is not tuned,\n",
    "                         # max_basis is set as number of basis\n",
    "\n",
    "# Regression Parameters \n",
    "# If a list is passed for any parameter automatic 5-fold CV is used to\n",
    "# determine the best parameter combination.\n",
    "params = {\"k\": 5}#[5, 10, 15, 20]}       # A dictionary with method-specific regression parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udbXVbiDZwie"
   },
   "source": [
    "Let's try first with a representative training/validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kb0yIUIjXSJm"
   },
   "outputs": [],
   "source": [
    "x_orig = pd.read_csv('test_set_photometry.csv')[['r', 'u-g', 'g-r', 'r-i', 'i-z']].to_numpy()\n",
    "y_orig = pd.read_csv('test_set_redshifts.csv')[['redshift']].to_numpy()\n",
    "posteriors_orig = pd.DataFrame(np.load('test_set_posteriors.csv')).to_numpy()\n",
    "\n",
    "# n_samp = 10000\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test, posteriors_train, posteriors_test = train_test_split(x_orig, y_orig, posteriors_orig, \n",
    "                                                                                       train_size=2000, random_state=42)\n",
    "x_train, x_validation, y_train, y_validation, posteriors_train, posteriors_validation = train_test_split(x_train, y_train, posteriors_train, \n",
    "                                                                                                         train_size=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lo6J8wY7pb1U"
   },
   "outputs": [],
   "source": [
    "# Parameterize model\n",
    "model = flexcode.FlexCodeModel(NN, max_basis, basis_system, regression_params=params)\n",
    "\n",
    "# Fit model - this will also choose the optimal number of neighbors `k`\n",
    "# %%time\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# # # Tune model - Select the best number of basis\n",
    "# # %%time\n",
    "model.tune(x_validation, y_validation)\n",
    "\n",
    "# # Predict new densities on grid\n",
    "# # %%time\n",
    "cde_test, y_grid = model.predict(x_test, n_grid=n_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "sQ8plrbNCoLn",
    "outputId": "7505e7b5-bccc-403c-8d70-f37ccd0e2794"
   },
   "outputs": [],
   "source": [
    "chosen = 9\n",
    "\n",
    "# granularity = 100\n",
    "# grid = np.linspace(z_min, means[column_idx]+3.*stds[column_idx], granularity)\n",
    "# pdf = flow.posterior(x_test[42], column=\"redshift\", grid=grid)\n",
    "\n",
    "plt.plot(y_grid, cde_test[chosen], label='Estimated posterior')\n",
    "plt.plot(grid, posteriors_test[chosen], label='True posterior')\n",
    "plt.axvline(y_test[chosen], 0, 1, c='C3', label='True redshift')\n",
    "plt.legend()\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAF62sfOawmd"
   },
   "source": [
    "Now let's try training and validating with some Happy/Teddy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWljb73yRRlT"
   },
   "outputs": [],
   "source": [
    "y_train = pd.read_csv('teddyAredshifts.csv')['redshift'].to_numpy()\n",
    "x_train = pd.read_csv('teddyAphotometry.csv')[['r', 'u-g', 'g-r', 'r-i', 'i-z']].to_numpy()\n",
    "\n",
    "y_validation = pd.read_csv('teddyBredshifts.csv')['redshift'].to_numpy()\n",
    "x_validation = pd.read_csv('teddyBphotometry.csv')[['r', 'u-g', 'g-r', 'r-i', 'i-z']].to_numpy()\n",
    "\n",
    "# x_test = pd.read_csv('test_set_photometry.csv')[['r', 'u-g', 'g-r', 'r-i', 'i-z']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmNhNgiIZp12"
   },
   "outputs": [],
   "source": [
    "# Parameterize model\n",
    "model = flexcode.FlexCodeModel(NN, max_basis, basis_system, regression_params=params)\n",
    "\n",
    "# Fit model - this will also choose the optimal number of neighbors `k`\n",
    "# %%time\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# # # Tune model - Select the best number of basis\n",
    "# # %%time\n",
    "model.tune(x_validation, y_validation)\n",
    "\n",
    "# # Predict new densities on grid\n",
    "# # %%time\n",
    "cde_test_bias, y_grid_bias = model.predict(x_test, n_grid=n_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ybeizm45J7H0"
   },
   "outputs": [],
   "source": [
    "# posteriors_test = pd.DataFrame(np.load('test_set_posteriors.csv')).to_numpy()\n",
    "# y_test = pd.read_csv('test_set_redshifts.csv')['redshift'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwnYgZL6aWUP"
   },
   "outputs": [],
   "source": [
    "chosen = 42\n",
    "\n",
    "# TODO: include one each from training, validation, and test sets\n",
    "\n",
    "plt.plot(y_grid_bias, cde_test_bias[chosen], label='Estimated posterior')\n",
    "plt.plot(grid, posteriors_test[chosen], label='True posterior')\n",
    "plt.axvline(y_test[chosen], 0, 1, c='C3', label='True redshift')\n",
    "plt.legend()\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cs_K8TENwDkn"
   },
   "source": [
    "### Your turn!\n",
    "\n",
    "AI is underutilized in estimating photo-$z$ posteriors, so the most exciting aspect of this challenge is to improve upon existing estimators.\n",
    "\n",
    "**CHALLENGE**: Implement another estimator of photo-$z$ posteriors; consider those used in Brian Nord's data challenge for inspiration.\n",
    "\n",
    "**CHALLENGE**: Devise and implement a way to obtain likelihoods rather than posteriors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fg4SzDIYiTGf"
   },
   "source": [
    "## 3. Evaluating the performance of estimated photo-$z$ posterior PDFs\n",
    "\n",
    "Once we have estimated photo-$z$ posterior PDFs, we need a way to determine if they're actually any good.\n",
    "Since the tutorial has only one method but multiple training/validation sets, that's all we can compare, but there are more options "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYd_evyhw3pC"
   },
   "source": [
    "### Metrics of estimated photo-$z$ posteriors and true redshifts\n",
    "\n",
    "First, let's try out a couple metrics of estimated photo-$z$ posteriors that do not require knowledge of the true photo-$z$ posteriors.\n",
    "**TODO: write out the definitions of CDE loss, PIT, HPD**\n",
    "**TODO: update code for metrics of cde-diagnostics**\n",
    "\n",
    "_This content is adapted from the [cde-diagnostics tutorial](https://github.com/zhao-david/CDE-diagnostics/blob/main/tutorial/tutorial-cde-diagnostics.ipynb), written by David Zhao (CMU)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L53A-iBubIvZ"
   },
   "outputs": [],
   "source": [
    "from cdetools import cde_loss, cdf_coverage, hpd_coverage\n",
    "# from cdetools.plot_utils import plot_with_uniform_band"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sVDo_DLgGit"
   },
   "source": [
    "The Probability Integral Transform (PIT) is defined as \n",
    "\\begin{equation}\n",
    "PIT = \\int_{-\\infty}^{z_{true}} p(z | \\vec{d}) dz .\n",
    "\\end{equation}\n",
    "A histogram of PIT values is commonly used to assess how consistent a population of photo-$z$ PDFs are with the true redshifts.\n",
    "Ideally, it would be a uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "uCWuAkGWk2ik",
    "outputId": "084df993-edb1-48f2-c743-5f4514e30217"
   },
   "outputs": [],
   "source": [
    "pit_values = 1. - cdf_coverage.cdf_coverage(cde_test, y_grid, y_test)\n",
    "# pit_values_bias = cdf_coverage.cdf_coverage(cde_test_bias, y_grid_bias, y_test)\n",
    "\n",
    "plt.hist(pit_values, alpha=0.5, bins=100, label='representative')\n",
    "# plt.hist(pit_values_bias, alpha=0.5, bins=100, label='biased')\n",
    "plt.legend()\n",
    "plt.xlabel('PIT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKxtSTGtlrbq"
   },
   "source": [
    "The Highest Posterior Density (HPD) is an extension of the PIT that similarly should be flat for an ideal case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84WUFQ8rfVtZ"
   },
   "outputs": [],
   "source": [
    "hpd_cov = hpd_coverage.hpd_coverage(cde_test, y_grid, y_test)\n",
    "# hpd_cov_bias = hpd_coverage.hpd_coverage(cde_test_bias, y_grid_bias, y_test)\n",
    "\n",
    "plt.hist(hpd_cov, alpha=0.5, bins=100, label='representative')\n",
    "# plt.hist(hpd_cov_bias, alpha=0.5, bins=100, label='biased')\n",
    "plt.legend()\n",
    "plt.xlabel('HPD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgkvMM8enL48"
   },
   "source": [
    "The CDE loss is defined in **TODO: link Schmidt & Malz here**. \n",
    "A lower value indicates a better estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQq6sv7JV4bH",
    "outputId": "3f0d78c8-9e80-4cc9-c353-ff96db968392"
   },
   "outputs": [],
   "source": [
    "cde_loss.cde_loss(cde_test, y_grid, y_test)\n",
    "# cde_loss.cde_loss(cde_test_bias, y_grid_bias, y_test)\n",
    "\n",
    "# global_pvalue = global_test(x_train=XZ_test, pit_train=pit_values,\n",
    "#                             x_test=grid,\n",
    "#                             alphas=np.linspace(0.0, 1.0, 11), clf_name='MLP', n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbTHU-y6xNKX"
   },
   "source": [
    "### Comparison of estimated and true photo-$z$ posterior PDFs\n",
    "\n",
    "There are two categories of metrics of approximated and true PDF: \n",
    "those that either rely upon or force the normalization condition $\\int p(z) dz = 1$and those that evaluate differences between arbitrary functions.\n",
    "`qp` **TODO: cite qp paper** is a package for manipulating univariate PDFs under many parameterizations and includes a few comparison metrics.\n",
    "The [original version](https://github.com/aimalz/qp) consistently enforced normalization but had limited functionality, whereas the [new version](https://github.com/LSSTDESC/qp) includes many more parameterizations whose usage is \"at your own risk\" in terms of possibly violating normalization.\n",
    "We'll use the new version for the sake of speed.\n",
    "\n",
    "_This content is adapted from the [qp demo](https://github.com/LSSTDESC/qp/blob/master/docs/notebooks/demo.ipynb), written by Alex Malz (GCCL@RUB), Phil Marshall (SLAC), and Eric Charles (SLAC)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-WARGQ6Gh-k_"
   },
   "outputs": [],
   "source": [
    "# P = qp.Ensemble(qp.interp, data=dict(xvals=grid.reshape(grid.shape[0]), yvals=posteriors_test))\n",
    "Q = qp.Ensemble(qp.interp, data=dict(xvals=y_grid.reshape(y_grid.shape[0]), yvals=cde_test))\n",
    "grid, approx_pdf_on_grid = Q.gridded(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioNxxbaDoDEm"
   },
   "source": [
    "The Kullback Leibler Divergence (KLD)\n",
    "\\begin{equation}\n",
    "KLD = \\int_{-\\infty}^{\\infty} p(z | \\vec{d}) \\log\\left[\\frac{p(z | \\vec{d}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi)}{p(z | \\vec{d})}\\right] dz\n",
    "\\end{equation}\n",
    "is a directional measure of how much information is lost by using the estimated $p(z | \\vec{d}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi)$ instead of the true $p(z | \\vec{d})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 942
    },
    "id": "J-J7YdTtLKPf",
    "outputId": "6d4c71d2-ec4c-4e68-b075-5b3ccd8382db"
   },
   "outputs": [],
   "source": [
    "KLDs = np.array([qp.metrics.quick_kld(p, q, dx=np.mean(grid[1:] - grid[:-1])) for p, q in zip(posteriors_test, approx_pdf_on_grid)])\n",
    "\n",
    "plt.hist(np.log(KLDs), bins=100)\n",
    "plt.xlabel('KLD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YR96wyRGPerk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "lrtkIQZXwlMZ",
    "outputId": "86de88d2-193e-411a-ea15-937400ca5c2d"
   },
   "outputs": [],
   "source": [
    "\n",
    "RMSEs = np.array([qp.metrics.quick_rmse(p, q, N=granularity) for p, q in zip(posteriors_test, approx_pdf_on_grid)])\n",
    "plt.hist(np.log(RMSEs), bins=100)\n",
    "plt.xlabel('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTwhvAxUxVnf"
   },
   "source": [
    "### Your turn!\n",
    "\n",
    "How else can we quantify the performance of estimators of aleatoric uncertainty?\n",
    "\n",
    "**CHALLENGE**: Apply and visualize the local metrics of [Zhao, Dalmasso, Izbicki & Lee, 2021](https://arxiv.org/abs/2102.10473), or any other metrics not included in this demo.\n",
    "\n",
    "**CHALLENGE**: Use the differences between estimated and true photo-$z$ posterior PDFs as a function of photometry to isolate the implicit prior of epistemic uncertainty from the aleatoric uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3lapDjSqtbx"
   },
   "source": [
    "# Your turn: How to participate in this challenge\n",
    "The organizers have identified three hack-able aspects to this data challenge:\n",
    "- Quantify sensitivity of ML photo-z posterior estimators to training set non-representativity\n",
    "- Implement and test additional AI methods for photo-z posterior estimation\n",
    "- Implement/interpret additional metrics of posterior precision\n",
    "\n",
    "Of course, there are many other possibilities for what to investigate based on this starting material!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DIwztiqPpgMU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lZsWWSsss9gd"
   ],
   "name": "data challenge prototype",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 - AI",
   "language": "python",
   "name": "python3-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
